{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e168883c-69ca-41f3-be58-20cb77fbd5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from rdflib import URIRef, OWL, Literal, RDF, RDFS, BNode\n",
    "from owlready2 import get_ontology\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils import *\n",
    "from src.noise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e60086c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OWL2DL-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb2b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = get_ontology(f'datasets/{dataset_name}.owl').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bfeb3",
   "metadata": {},
   "source": [
    "**Filter unnecessary triples from inferred graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_properties = list(ontology.object_properties())\n",
    "object_properties = [URIRef(x.iri) for x in object_properties]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b69d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_inferred_triples(input_path, inferred_path, filtered_inferred_path):  \n",
    "    g = rdflib.Graph()\n",
    "    g.parse(input_path)  \n",
    "    g_inferred = rdflib.Graph()\n",
    "    g_inferred.parse(inferred_path)\n",
    "\n",
    "    g_inferred_filtered = rdflib.Graph()\n",
    "    for triple in g_inferred: \n",
    "        if ((triple[1] in object_properties or triple[1] in {RDFS.subClassOf, RDF.type}) and \n",
    "            not isinstance(triple[2], Literal) and                           \n",
    "            triple[2] != OWL.Thing and                                 \n",
    "            triple not in g):\n",
    "            g_inferred_filtered.add(triple)\n",
    "    g_inferred_filtered.serialize(filtered_inferred_path, format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eef997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_graph_path in tqdm(sorted(glob('datasets/inferred_graphs/' + \"*\"))):\n",
    "    file = input_graph_path.replace('datasets/inferred_graphs\\\\', '').replace('.xml', '')\n",
    "    filter_inferred_triples('datasets/input_graphs/' + file + '.ttl', \n",
    "                            input_graph_path, \n",
    "                            'datasets/inferred_graphs_filtered/' + file + '.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d70fcc",
   "metadata": {},
   "source": [
    "**Create train, test, val sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_type(s):\n",
    "    s = s.split('\\\\')[-1]\n",
    "    s = s.split('_')[:-1]\n",
    "    s = \"_\".join(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf21091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_df(INPUT_GRAPHS_FOLDER, INFERENCE_GRAPHS_FOLDER):\n",
    "    logging.info(f\"Creating dataframe for {dataset_name} input/inference pairs\")\n",
    "    rdf_files = []\n",
    "    for input_graph_path in tqdm(sorted(glob(INPUT_GRAPHS_FOLDER + \"*\"))):\n",
    "        input_graph_file = os.path.basename(input_graph_path)\n",
    "        inference_path = INFERENCE_GRAPHS_FOLDER + input_graph_file\n",
    "        graph_type = get_graph_type(input_graph_path)\n",
    "        rdf_pair = {\"input_graph_file\": input_graph_path, \"inference_file\": inference_path, \"graph_type\": graph_type}\n",
    "        rdf_files.append(rdf_pair)\n",
    "    files_df = pd.DataFrame.from_dict(rdf_files)\n",
    "    return files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = get_files_df('datasets/input_graphs_filtered_1hop/', 'datasets/inferred_graphs_filtered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classes for which only one instance exists\n",
    "df_count = pd.DataFrame(files_df['graph_type'].value_counts())\n",
    "graph_type_2_keep = df_count[df_count['graph_type'] > 1].index\n",
    "files_df = files_df[files_df['graph_type'].isin(graph_type_2_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=0.6, validate_percent=0.2, stratify=None, seed=1):\n",
    "    val_test_percent = 1 - train_percent\n",
    "    test_percent = (1 - (train_percent + validate_percent))\n",
    "    test_percent = test_percent / (test_percent + validate_percent)\n",
    "    if stratify:\n",
    "        df_train, df_val_test = train_test_split(df, test_size=val_test_percent, random_state=seed,\n",
    "                                                 stratify=df[stratify])\n",
    "        df_val, df_test = train_test_split(df_val_test, test_size=test_percent, random_state=seed,\n",
    "                                           stratify=df_val_test[stratify])\n",
    "    else:\n",
    "        df_train, df_val_test = train_test_split(df, test_size=val_test_percent, random_state=seed)\n",
    "        df_val, df_test = train_test_split(df_val_test, test_size=test_percent, random_state=seed)\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7790f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_data_train, rdf_data_val, rdf_data_test = train_validate_test_split(files_df,\n",
    "                                                                        train_percent=0.6,\n",
    "                                                                        validate_percent=0.2,\n",
    "                                                                        stratify=\"graph_type\",\n",
    "                                                                        seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nt_files(data, output_file):\n",
    "    merged_graph = rdflib.Graph()\n",
    "\n",
    "    nt_files_orig = data.get('input_graph_file', pd.Series(dtype=str))\n",
    "    nt_files_inferred = data.get('inference_file', pd.Series(dtype=str))\n",
    "    \n",
    "    nt_files = pd.concat([nt_files_orig, nt_files_inferred], ignore_index=True)\n",
    "\n",
    "    for nt_file in nt_files:\n",
    "        try:\n",
    "            graph = rdflib.Graph()\n",
    "            if 'TBOX' in nt_file: graph.parse(nt_file)\n",
    "            else: graph.parse(nt_file, format=\"turtle\")\n",
    "            merged_graph += graph  \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not parse {nt_file} - {e}\")\n",
    "\n",
    "    merged_graph.serialize(destination=output_file)\n",
    "    print(f\"Merged file created at {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nt_files(rdf_data_train, f'datasets/{dataset_name}_train_complete.owl')\n",
    "merge_nt_files(rdf_data_val, f'datasets/{dataset_name}_val_complete.owl')\n",
    "merge_nt_files(rdf_data_test, f'datasets/{dataset_name}_test_complete.owl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b8b88",
   "metadata": {},
   "source": [
    "**Manage duplicates and drop BNodes in train, test and val sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_data_train['resource'] = rdf_data_train['input_graph_file'].str.extract(r'_([^_]+)\\.ttl$')\n",
    "rdf_data_test['resource'] = rdf_data_test['input_graph_file'].str.extract(r'_([^_]+)\\.ttl$')\n",
    "rdf_data_val['resource'] = rdf_data_val['input_graph_file'].str.extract(r'_([^_]+)\\.ttl$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182353e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train = rdflib.Graph()\n",
    "G_train.parse(f'datasets/{dataset_name}_train_complete.owl', format='turtle')\n",
    "print(f'# triples in G_train: {len(G_train)}')\n",
    "\n",
    "G_test = rdflib.Graph()\n",
    "G_test.parse(f'datasets/{dataset_name}_test_complete.owl', format='turtle')\n",
    "print(f'# triples in G_test: {len(G_test)}')\n",
    "\n",
    "G_val = rdflib.Graph()\n",
    "G_val.parse(f'datasets/{dataset_name}_val_complete.owl', format='turtle')\n",
    "print(f'# triples in G_val: {len(G_val)}')\n",
    "\n",
    "G_tbox = rdflib.Graph()\n",
    "G_tbox.parse(f'datasets/{dataset_name}_TBOX.owl')\n",
    "print(f'# triples in TBox: {len(G_tbox)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_train_unique = set(G_train) - set(G_test) - set(G_val) \n",
    "G_test_unique = set(G_test) - set(G_train) - set(G_val) \n",
    "G_val_unique = set(G_val) - set(G_train) - set(G_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509669ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_duplicates_to_set(G1, G2):    \n",
    "    intersection = G1 & G2\n",
    "\n",
    "    df_intersection = pd.DataFrame(list(intersection), columns=[\"subject\", \"predicate\", \"object\"])\n",
    "    df_intersection['subject_name'] = df_intersection['subject'].apply(lambda x: x.replace('https://kracr.iiitd.edu.in/OWL2Bench#', ''))\n",
    "    df_intersection['source'] = (df_intersection['subject_name'].apply(lambda x: ', '.join([name for name, df in [('train', rdf_data_train), ('test', rdf_data_test), ('val', rdf_data_val)]\n",
    "                                                if x in df['resource'].values])))\n",
    "    df_intersection['source'] = df_intersection['source'].apply(lambda x: 'test' if x == '' else x)\n",
    "\n",
    "    set_train = set(df_intersection[df_intersection['source'] == 'train'].apply(lambda row: (row['subject'], row['predicate'], row['object']), axis=1))\n",
    "    set_test = set(df_intersection[df_intersection['source'] == 'test'].apply(lambda row: (row['subject'], row['predicate'], row['object']), axis=1))\n",
    "    set_val = set(df_intersection[df_intersection['source'] == 'val'].apply(lambda row: (row['subject'], row['predicate'], row['object']), axis=1))\n",
    "\n",
    "    return set_train, set_test, set_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba494ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_train, set_test, set_val = add_duplicates_to_set(G_train, G_test)\n",
    "G_train_unique.update(set_train)\n",
    "G_test_unique.update(set_test)\n",
    "G_val_unique.update(set_val)\n",
    "\n",
    "set_train, set_test, set_val = add_duplicates_to_set(G_train, G_val)\n",
    "G_train_unique.update(set_train)\n",
    "G_test_unique.update(set_test)\n",
    "G_val_unique.update(set_val)\n",
    "\n",
    "set_train, set_test, set_val = add_duplicates_to_set(G_test, G_val)\n",
    "G_train_unique.update(set_train)\n",
    "G_test_unique.update(set_test)\n",
    "G_val_unique.update(set_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bnodes(graph):\n",
    "    new_graph = rdflib.Graph()\n",
    "    for s, p, o in graph:\n",
    "        if isinstance(s, BNode) or isinstance(p, BNode) or isinstance(o, BNode):\n",
    "            continue  \n",
    "        new_graph.add((s, p, o))\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_G_train = remove_bnodes(G_train_unique)\n",
    "filtered_G_test = remove_bnodes(G_test_unique)\n",
    "filtered_G_val = remove_bnodes(G_val_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cecb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_G_train += G_tbox\n",
    "filtered_G_test += G_tbox\n",
    "filtered_G_val += G_tbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_G_train.serialize(f'datasets/{dataset_name}_train.owl')\n",
    "filtered_G_test.serialize(f'datasets/{dataset_name}_test.owl')\n",
    "filtered_G_val.serialize(f'datasets/{dataset_name}_val.owl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
