{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168883c-69ca-41f3-be58-20cb77fbd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rdflib\n",
    "from rdflib import URIRef, OWL, Literal\n",
    "from owlready2 import get_ontology\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60086c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OWL2DL-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bfeb3",
   "metadata": {},
   "source": [
    "**Filter unnecessary triples from g and i**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = get_ontology(f'datasets/{dataset_name}.owl').load()\n",
    "subject_resources = list(ontology.individuals())\n",
    "named_individuals = [URIRef(x.iri) for x in subject_resources]\n",
    "print(f'# Subject-Resources: {len(subject_resources)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b69d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_inferred_triples(input_path, inferred_path, filtered_inferred_path):  \n",
    "    g = rdflib.Graph()\n",
    "    g.parse(input_path)  \n",
    "    g_inferred = rdflib.Graph()\n",
    "    g_inferred.parse(inferred_path)\n",
    "\n",
    "    g_inferred_filtered = rdflib.Graph()\n",
    "    for triple in g_inferred: \n",
    "        if (#(triple[0] in named_individuals or triple[2] in named_individuals) and \n",
    "            not isinstance(triple[2], Literal) and                           \n",
    "            triple[2] != OWL.Thing and                                 \n",
    "            triple not in g):\n",
    "            g_inferred_filtered.add(triple)\n",
    "    g_inferred_filtered.serialize(filtered_inferred_path, format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eef997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_graph_path in tqdm(sorted(glob('datasets/inferred_graphs/' + \"*\"))):\n",
    "    file = input_graph_path.replace('datasets/inferred_graphs\\\\', '').replace('.xml', '')\n",
    "    filter_inferred_triples('datasets/input_graphs/' + file + '.ttl', \n",
    "                            input_graph_path, \n",
    "                            'datasets/inferred_graphs_filtered/' + file + '.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d70fcc",
   "metadata": {},
   "source": [
    "**Create train, test, val sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95250b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_type(s):\n",
    "    s = s.split('/')[-1]\n",
    "    s = s.split('_')[:-1]\n",
    "    s = \"_\".join(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff4939-7885-4f6d-98b4-d011f54210d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_df(INPUT_GRAPHS_FOLDER, INFERENCE_GRAPHS_FOLDER):\n",
    "    logging.info(f\"Creating dataframe for {dataset_name} input/inference pairs\")\n",
    "    rdf_files = []\n",
    "    for input_graph_path in tqdm(sorted(glob(INPUT_GRAPHS_FOLDER + \"*\"))):\n",
    "        input_graph_file = os.path.basename(input_graph_path)\n",
    "        inference_path = INFERENCE_GRAPHS_FOLDER + input_graph_file.replace('.ttl','_inferred.ttl')\n",
    "        graph_type = get_graph_type(input_graph_path)\n",
    "        rdf_pair = {\"input_graph_file\": input_graph_path, \"inference_file\": inference_path, \"graph_type\": graph_type}\n",
    "        rdf_files.append(rdf_pair)\n",
    "    files_df = pd.DataFrame.from_dict(rdf_files)\n",
    "    return files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609afd2-3d09-4ad1-b397-55b8bb44559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = get_files_df('MyJenaProject/input_filtered/', 'MyJenaProject/output_filtered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classes for which only one instance exists\n",
    "df_count = pd.DataFrame(files_df['graph_type'].value_counts())\n",
    "graph_type_2_keep = df_count[df_count['count'] > 1].index\n",
    "files_df = files_df[files_df['graph_type'].isin(graph_type_2_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830f0dc-339f-4454-a3f5-31ac8a29dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=0.6, validate_percent=0.2, stratify=None, seed=1):\n",
    "    val_test_percent = 1 - train_percent\n",
    "    test_percent = (1 - (train_percent + validate_percent))\n",
    "    test_percent = test_percent / (test_percent + validate_percent)\n",
    "    if stratify:\n",
    "        df_train, df_val_test = train_test_split(df, test_size=val_test_percent, random_state=seed,\n",
    "                                                 stratify=df[stratify])\n",
    "        df_val, df_test = train_test_split(df_val_test, test_size=test_percent, random_state=seed,\n",
    "                                           stratify=df_val_test[stratify])\n",
    "    else:\n",
    "        df_train, df_val_test = train_test_split(df, test_size=val_test_percent, random_state=seed)\n",
    "        df_val, df_test = train_test_split(df_val_test, test_size=test_percent, random_state=seed)\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959ea23-2a80-4381-958c-49fbe2a079c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_data_train, rdf_data_val, rdf_data_test = train_validate_test_split(files_df,\n",
    "                                                                        train_percent=0.6,\n",
    "                                                                        validate_percent=0.2,\n",
    "                                                                        stratify=\"graph_type\",\n",
    "                                                                        seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9886c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nt_files_with_rdflib(data, output_file, old_iri, new_iri):\n",
    "    merged_graph = rdflib.Graph()\n",
    "\n",
    "    nt_files_orig = data.get('input_graph_file', pd.Series(dtype=str))\n",
    "    nt_files_inferred = data.get('inference_file', pd.Series(dtype=str))\n",
    "    tbox_path = pd.Series([f'datasets/{dataset_name}_TBOX.owl'])\n",
    "    \n",
    "    nt_files = pd.concat([nt_files_orig, nt_files_inferred, tbox_path], ignore_index=True)\n",
    "\n",
    "    for nt_file in nt_files:\n",
    "        try:\n",
    "            graph = rdflib.Graph()\n",
    "            if 'TBOX' in nt_file: graph.parse(nt_file)\n",
    "            else: graph.parse(nt_file, format=\"turtle\")\n",
    "            if old_iri and new_iri:\n",
    "                for s, p, o in graph:\n",
    "                    s = URIRef(str(s).replace(old_iri, new_iri)) if isinstance(s, URIRef) else s\n",
    "                    p = URIRef(str(p).replace(old_iri, new_iri)) if isinstance(p, URIRef) else p\n",
    "                    o = URIRef(str(o).replace(old_iri, new_iri)) if isinstance(o, URIRef) else o\n",
    "                    merged_graph.add((s, p, o))\n",
    "            else:\n",
    "                merged_graph += graph  \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not parse {nt_file} - {e}\")\n",
    "\n",
    "    merged_graph.serialize(destination=output_file)\n",
    "    print(f\"Merged file created at {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c58b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nt_files_with_rdflib(rdf_data_train, f'datasets/bin/{dataset_name}_train.owl', \n",
    "              'https://kracr.iiitd.edu.in/OWL2Bench', 'https://kracr.iiitd.edu.in/OWL2Bench_train')\n",
    "merge_nt_files_with_rdflib(rdf_data_val, f'datasets/bin/{dataset_name}_val.owl', \n",
    "              'https://kracr.iiitd.edu.in/OWL2Bench', 'https://kracr.iiitd.edu.in/OWL2Bench_val')\n",
    "merge_nt_files_with_rdflib(rdf_data_test, f'datasets/bin/{dataset_name}_test.owl', \n",
    "              'https://kracr.iiitd.edu.in/OWL2Bench', 'https://kracr.iiitd.edu.in/OWL2Bench_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651feb78",
   "metadata": {},
   "source": [
    "**Add noise to training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c92bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = get_experimets(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ce8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments[1:]: \n",
    "    dataset_name = experiment['dataset_name']\n",
    "    file_name = experiment['file_name']\n",
    "    format_ = experiment['format_']\n",
    "    add_noise = experiment['add_noise']  \n",
    "\n",
    "    g_train = rdflib.Graph()\n",
    "    g_train.parse(f'datasets/bin/{dataset_name}_train.owl', format='turtle')\n",
    "    print(f'# G_train: {len(g_train)}')\n",
    "\n",
    "    g_noise = rdflib.Graph()\n",
    "    g_noise.parse(f'datasets/noise/{file_name}.owl')\n",
    "    print(f'# G_noise: {len(g_noise)}')\n",
    "\n",
    "    g_train += g_noise\n",
    "    g_train.serialize(destination=f'datasets/bin/{file_name}_train.owl')\n",
    "    print(f'# G_train + G_noise: {len(g_train)}')\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
