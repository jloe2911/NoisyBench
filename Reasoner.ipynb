{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47422e52-7ae7-4afc-a3f3-20cf7a20fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import rdflib\n",
    "\n",
    "import mowl\n",
    "mowl.init_jvm('10g')\n",
    "from mowl.datasets import PathDataset\n",
    "from mowl.projection import OWL2VecStarProjector\n",
    "from mowl.projection.edge import Edge\n",
    "from mowl.walking import DeepWalk\n",
    "from mowl.kge import KGEModel\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "from src.gnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa895ae4-b0ec-4c68-985d-cd1823f306db",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [{'file_name' : 'family',\n",
    "                'format_' : None},\n",
    "               {'file_name' : 'family_noisy_gnn_100',\n",
    "                'format_' : 'ttl'},\n",
    "               {'file_name' : 'family_noisy_gnn_1000',\n",
    "                'format_' : 'ttl'},\n",
    "               #{'file_name' : 'family_noisy_gnn_10000',\n",
    "               # 'format_' : 'ttl'},\n",
    "               #{'file_name' : 'family_noisy_gnn_100000',\n",
    "               # 'format_' : 'ttl'},   \n",
    "               {'file_name' : 'family_noisy_random_100',\n",
    "                'format_' : 'ttl'},\n",
    "               {'file_name' : 'family_noisy_random_1000',\n",
    "                'format_' : 'ttl'}#,\n",
    "               #{'file_name' : 'family_noisy_random_10000',\n",
    "               # 'format_' : 'ttl'},\n",
    "               #{'file_name' : 'family_noisy_random_100000',\n",
    "               #'format_' : 'ttl'}\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fdc678-638d-44a5-8287-8201b24e2429",
   "metadata": {},
   "source": [
    "# 1. Split ontology into train/test ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04371f18-dec9-4435-8df5-9f9c60a72270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ontology(file_name, format_, train_ratio):\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(f'datasets/{file_name}.owl', format=format_)  \n",
    "    print(f'Triplets found: %d' % len(g))\n",
    "\n",
    "    triples = list(g.triples((None, None, None))) \n",
    "    random.shuffle(triples) \n",
    "\n",
    "    split_index = int(train_ratio * len(triples))\n",
    "\n",
    "    train_triples = triples[:split_index]\n",
    "    test_triples = triples[split_index:]\n",
    "\n",
    "    train_graph = rdflib.Graph()\n",
    "    test_graph = rdflib.Graph()\n",
    "\n",
    "    for triple in train_triples:\n",
    "        train_graph.add(triple)\n",
    "\n",
    "    for triple in test_triples:\n",
    "        test_graph.add(triple)\n",
    "\n",
    "    print(f'Train Triplets found: %d' % len(train_graph))\n",
    "    train_graph.serialize(destination=f\"datasets/bin/{file_name}_train.owl\")\n",
    "    print(f'Test Triplets found: %d' % len(test_graph))\n",
    "    test_graph.serialize(destination=f\"datasets/bin/{file_name}_test.owl\")\n",
    "    \n",
    "    return train_graph, test_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22612b87-2ffd-447b-96da-be028821cbd5",
   "metadata": {},
   "source": [
    "# 2. OWL2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c12e9-b5da-44ae-9ca5-4b5162b62af7",
   "metadata": {},
   "source": [
    "**Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bf3b66-bb2c-41b9-9c02-838512217d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def owl2vec_fit(file_name, embed_dim, load):\n",
    "    dataset = PathDataset(ontology_path=f'datasets/bin/{file_name}_train.owl',\n",
    "                          testing_path=f'datasets/bin/{file_name}_test.owl')\n",
    "    if not load:\n",
    "        projector = OWL2VecStarProjector(bidirectional_taxonomy=True)\n",
    "        edges = projector.project(dataset.ontology)\n",
    "        walker = DeepWalk(num_walks=20,\n",
    "                          walk_length=20,\n",
    "                          alpha=0.1,\n",
    "                          workers=4)         \n",
    "        walks = walker.walk(edges)\n",
    "        sentences = LineSentence(walker.outfile)\n",
    "        model = Word2Vec(sentences, vector_size=embed_dim, epochs=300, window=5, min_count=1, workers=4)\n",
    "        model.save(f'models/owl2vec_{file_name}.model')\n",
    "    else:\n",
    "        model = Word2Vec.load(f'models/owl2vec_{file_name}.model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eeeed3-6c8b-4f3b-bf8e-88a091f782df",
   "metadata": {},
   "source": [
    "**Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f87119-1628-4331-8191-d842c7772a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def owl2vec_eval(owl2vec_model, test_graph):\n",
    "    vectors = owl2vec_model.wv\n",
    "    words = list(owl2vec_model.wv.key_to_index)\n",
    "    output_owl2vec = torch.tensor(vectors[words])\n",
    "    \n",
    "    nodes = list(set(words))\n",
    "    nodes_dict = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    i=0\n",
    "    edge_data = defaultdict(list)\n",
    "    for s, p, o in test_graph.triples((None, None, None)):\n",
    "        s = s.n3()\n",
    "        s = s.replace('<','')\n",
    "        s = s.replace('>','')\n",
    "        o = o.n3()\n",
    "        o = o.replace('<','')\n",
    "        o = o.replace('>','')\n",
    "        try:\n",
    "            src, dst = nodes_dict[s], nodes_dict[o]\n",
    "            edge_data['edge_index'].append([src, dst])\n",
    "        except:\n",
    "            i+=1\n",
    "    edge_index = torch.tensor(edge_data['edge_index']).reshape(2,-1)\n",
    "    \n",
    "    hits1, hits10 = eval_hits(edge_index=edge_index,\n",
    "                              tail_pred=1,\n",
    "                              output=output_owl2vec,\n",
    "                              max_num=100)\n",
    "    print(f'Hits@1: {hits1:.3f}, Hits@10: {hits10:.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625a210-4e24-44da-bdd2-b892e3cd4325",
   "metadata": {},
   "source": [
    "**Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04bfe98-f759-46fa-883a-3df2b6af61eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family\n",
      "Triplets found: 5017\n",
      "Train Triplets found: 4013\n",
      "Test Triplets found: 1004\n",
      "Hits@1: 0.021, Hits@10: 0.142\n",
      "\n",
      "family_noisy_gnn_100\n",
      "Triplets found: 5817\n",
      "Train Triplets found: 4653\n",
      "Test Triplets found: 1164\n",
      "Hits@1: 0.009, Hits@10: 0.112\n",
      "\n",
      "family_noisy_gnn_1000\n",
      "Triplets found: 13015\n",
      "Train Triplets found: 10412\n",
      "Test Triplets found: 2603\n",
      "Hits@1: 0.046, Hits@10: 0.342\n",
      "\n",
      "family_noisy_random_100\n",
      "Triplets found: 5806\n",
      "Train Triplets found: 4644\n",
      "Test Triplets found: 1162\n",
      "Hits@1: 0.035, Hits@10: 0.221\n",
      "\n",
      "family_noisy_random_1000\n",
      "Triplets found: 12861\n",
      "Train Triplets found: 10288\n",
      "Test Triplets found: 2573\n",
      "Hits@1: 0.081, Hits@10: 0.459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for experiment in experiments: \n",
    "    file_name = experiment['file_name']\n",
    "    print(file_name)\n",
    "    format_ = experiment['format_']\n",
    "\n",
    "    train_graph, test_graph = split_ontology(file_name=file_name, format_=format_, train_ratio=0.8)\n",
    "    owl2vec_model = owl2vec_fit(file_name=file_name, embed_dim=200, load=False)\n",
    "    owl2vec_eval(owl2vec_model, test_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
