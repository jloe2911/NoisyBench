{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761fa5f2-c0ad-4fea-84bc-6ecbcf759d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import rdflib\n",
    "from rdflib import Namespace, Literal\n",
    "from owlready2 import get_ontology\n",
    "\n",
    "from src.utils import *\n",
    "from src.gnn import *\n",
    "from src.sparql_queries import *\n",
    "from src.noise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f479add1-d4dd-44ff-9119-cd18dd1edd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51ef75-efba-4f30-8032-42ed4b0fc980",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'family'\n",
    "# dataset_name = 'pizza'\n",
    "# dataset_name = 'OWL2DL-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7b16e0-0fa3-4d0a-988d-4d2f1bc3cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'family':\n",
    "    uri = Namespace(\"http://www.co-ode.org/roberts/family-tree.owl#\")\n",
    "elif dataset_name == 'pizza':\n",
    "    uri = Namespace(\"http://www.co-ode.org/ontologies/pizza/pizza.owl#\")\n",
    "elif dataset_name.startswith('OWL2DL-'):\n",
    "    uri = Namespace(\"http://benchmark/OWL2Bench#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df7d31-b741-44b4-a997-e17df3218285",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182adee3-c4ac-4a76-9897-5b17c7f42b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplets found in OWL2DL-1.owl: 55215\n",
      "Triplets found in OWL2DL-1_train.owl: 40588\n"
     ]
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse(f'datasets/{dataset_name}.owl')\n",
    "num_triples = len(g)\n",
    "print(f'Triplets found in {dataset_name}.owl: %d' % num_triples)\n",
    "\n",
    "g_no_noise = rdflib.Graph()\n",
    "g_no_noise.parse(f'datasets/bin/{dataset_name}_train.owl')\n",
    "num_triples_train = len(g_no_noise)\n",
    "print(f'Triplets found in {dataset_name}_train.owl: %d' % num_triples_train)\n",
    "\n",
    "ontology = get_ontology(f'datasets/{dataset_name}.owl').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd34f1e-e8ed-41cb-ade2-598e51112aba",
   "metadata": {},
   "source": [
    "# 2. GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b0c3a-2379-4ce2-8c29-f75430d64d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relations_dict = {rel: i for i, rel in enumerate(relations)}\n",
    "# nodes_dict = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# nodes_dict_rev = {value: key for key, value in nodes_dict.items()}\n",
    "# relations_dict_rev = {value: key for key, value in relations_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51d2d9a-ca9a-49bc-a6ce-de64cb017415",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(g_no_noise)\n",
    "data = split_edges(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c9a71d9-df0a-4987-a2ad-eded19f4b5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  edge_index=[2, 55215],\n",
       "  edge_type=[55215],\n",
       "  val_pos_edge_index=[2, 0],\n",
       "  val_edge_type=[0],\n",
       "  test_pos_edge_index=[2, 11043],\n",
       "  test_edge_type=[11043],\n",
       "  train_pos_edge_index=[2, 44172],\n",
       "  train_edge_type=[44172]\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c27ccc-782b-4e6f-813f-10a72e01f989",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424d0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list(set(g_no_noise.predicates()))\n",
    "nodes = list(set(g_no_noise.subjects()).union(set(g_no_noise.objects())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad75650-5fe6-4014-be3b-8c90272091bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "model = GNN(device, len(nodes), len(relations))\n",
    "\n",
    "for epoch in range(10+1):\n",
    "    loss = model._train(data.to(device))\n",
    "    print(f'Epoch: {epoch}, Loss: {loss:.4f}')\n",
    "\n",
    "torch.save(model, f'models/RGCN_{dataset_name}_train')\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print(f'Run time: {elapsed_time:.0f} seconds, {elapsed_time/60:.0f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3695fb2-fc34-4c40-acec-10cf70f9a7e9",
   "metadata": {},
   "source": [
    "**Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "477909fa-bce0-499c-bd3a-98f42897aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(f'models/RGCN_{dataset_name}_train')\n",
    "# mrr, mean_rank, median_rank, hits_at_5, hits_at_10 = model._eval(data.to(device))\n",
    "# print(f'MRR: {mrr:.3f}, Mean Rank: {mean_rank:.3f}, Median Rank: {median_rank:.3f}, Hits@5: {hits_at_5:.3f}, Hits@10: {hits_at_10:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e038ffe5-3423-49df-bd62-f69964b062d4",
   "metadata": {},
   "source": [
    "# 3. Noise Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae62b8-d00d-4593-b777-cbc253abf2e0",
   "metadata": {},
   "source": [
    "## 3.1. GNN: we add k triples with a low prediction score to the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4dcc0c-2c24-499b-a3b3-77bc5814aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_triples_gnn(g, data, edge_types, noise_percentage):\n",
    "    k = int((noise_percentage * num_triples) / len(edge_types))\n",
    "    noisy_g_gnn = rdflib.Graph()\n",
    "    new_g_gnn = copy_graph(g)\n",
    "    for etype in tqdm(edge_types):   \n",
    "        mask = data.edge_type == etype\n",
    "        edge_index = torch.tensor([data.edge_index[0,mask].tolist(),data.edge_index[1,mask].tolist()])\n",
    "        edge_type = data.edge_type[mask]\n",
    "\n",
    "        output = model.model.encode(edge_index.to(device), edge_type.to(device))\n",
    "\n",
    "        link_pred_scores = torch.matmul(output, output.T)\n",
    "        output_norm = torch.norm(output, dim=1, keepdim=True)\n",
    "        link_pred_scores_norm = link_pred_scores / (output_norm * output_norm.T)\n",
    "        \n",
    "        # We do not want to generate links that already exists\n",
    "        # We want the subject and object to be an individual \n",
    "        link_pred_scores_norm[edge_index[0,:],edge_index[1,:]] = 1\n",
    "        subset = link_pred_scores_norm[individual_id][:, individual_id]\n",
    "\n",
    "        # Find the indices of the top k smallest elements\n",
    "        _, topk_indices = torch.topk(subset.flatten(), k*2, largest=False)\n",
    "        row_indices = topk_indices // subset.size(1)\n",
    "        col_indices = topk_indices % subset.size(1)\n",
    "\n",
    "        # Filter out indices where row index is greater than column index\n",
    "        valid_indices_mask = row_indices < col_indices\n",
    "        row_indices = row_indices[valid_indices_mask]\n",
    "        col_indices = col_indices[valid_indices_mask]\n",
    "        \n",
    "        # Add generated triples\n",
    "        node1_lst = [individual_names_dict[key] for key in row_indices.tolist()]\n",
    "        node2_lst = [individual_names_dict[key] for key in col_indices.tolist()]\n",
    "        edge_type_uri = relations_dict_rev[etype]\n",
    "        noisy_g_gnn = add_links(noisy_g_gnn, node1_lst, node2_lst, edge_type_uri)\n",
    "        new_g_gnn = add_links(new_g_gnn, node1_lst, node2_lst, edge_type_uri)\n",
    "        \n",
    "    return noisy_g_gnn, new_g_gnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da772b78-efc7-4218-867e-fce6407fae7a",
   "metadata": {},
   "source": [
    "## 3.2. Random: we add k random triples to the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ff7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_triples_random(g_no_noise, uri, noise_percentage):\n",
    "    max_triples = int(noise_percentage * len(g_no_noise)) \n",
    "\n",
    "    noisy_g_random = rdflib.Graph()\n",
    "    new_g_random = copy_graph(g_no_noise)\n",
    "    num_triples = 0\n",
    "\n",
    "    possible_predicates = get_possible_predicates(g_no_noise)\n",
    "    subjects, objects = get_subjects_objects_given_predicate(g_no_noise, [str(uri).split(\"#\")[-1] for uri in possible_predicates], uri)\n",
    "\n",
    "    while num_triples < max_triples:\n",
    "        s = random.choice(subjects)\n",
    "        p = random.choice(possible_predicates)\n",
    "        o = random.choice(objects)\n",
    "\n",
    "        triple = (s, URIRef(uri + p), o)\n",
    "\n",
    "        if triple not in g_no_noise:\n",
    "            noisy_g_random.add(triple)\n",
    "            new_g_random.add(triple)\n",
    "            num_triples += 1\n",
    "    return noisy_g_random, new_g_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63efccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "noisy_g_random, new_g_random = add_triples_random(g_no_noise, uri, 0.25)\n",
    "print(0.25 * len(g_no_noise))\n",
    "print(len(noisy_g_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc41e03-83f1-47a9-bb9e-db0f20d6fe18",
   "metadata": {},
   "source": [
    "## 3.3. DL: we add individuals to the ontology that belong to disjoint classes/properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982897fe-4e91-41f7-be06-d4066d0bdc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_disjoint_classes = get_disjoint_classes(ontology)\n",
    "all_disjoint_properties = get_disjoint_properties(ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e698fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e422d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_disjoint_axioms(g, g_no_noise, all_disjoint_classes, all_disjoint_properties, uri, noise_percentage):    \n",
    "    max_triples = int((noise_percentage * len(g_no_noise))/2)\n",
    "\n",
    "    noisy_g_disjoint = rdflib.Graph()\n",
    "    noisy_g_disjoint += add_noise_disjoint_classes(g_no_noise, max_triples, all_disjoint_classes, uri)\n",
    "    noisy_g_disjoint += add_noise_disjoint_properties(g, g_no_noise, max_triples, all_disjoint_properties, uri)\n",
    "\n",
    "    new_g_disjoint = copy_graph(g_no_noise)\n",
    "    new_g_disjoint += add_noise_disjoint_classes(g_no_noise, max_triples, all_disjoint_classes, uri)\n",
    "    new_g_disjoint += add_noise_disjoint_properties(g, g_no_noise, max_triples, all_disjoint_properties, uri)\n",
    "    return noisy_g_disjoint, new_g_disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "noisy_g_disjoint, new_g_disjoint = add_disjoint_axioms(g, g_no_noise, all_disjoint_classes, all_disjoint_properties, uri, 0.25)\n",
    "print(0.25 * len(g_no_noise))\n",
    "print(len(noisy_g_disjoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676690b-4924-4045-9932-7e0963792cdf",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe72a515-07c1-4780-8c25-0e4372e68e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2232282/953067345.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f'models/RGCN_{dataset_name}')\n",
      "100%|██████████| 38/38 [01:09<00:00,  1.84s/it]\n",
      "100%|██████████| 38/38 [00:00<00:00, 58.56it/s]\n",
      "100%|██████████| 38/38 [01:14<00:00,  1.97s/it]\n",
      "100%|██████████| 38/38 [00:01<00:00, 32.87it/s]\n",
      "100%|██████████| 38/38 [01:11<00:00,  1.88s/it]\n",
      "100%|██████████| 38/38 [00:02<00:00, 15.92it/s]\n",
      "100%|██████████| 38/38 [01:32<00:00,  2.42s/it]\n",
      "100%|██████████| 38/38 [00:02<00:00, 16.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(f'models/RGCN_{dataset_name}')\n",
    "\n",
    "for noise_percentage in [0.25, 0.5, 0.75, 1.0]:\n",
    "\n",
    "    noisy_g_gnn, new_g_gnn = add_triples_gnn(g, data, edge_types, noise_percentage)\n",
    "    noisy_g_gnn.serialize(destination=f\"datasets/noise/{dataset_name}_gnn_{noise_percentage}.owl\", format='xml')\n",
    "    \n",
    "    noisy_g_random, new_g_random = add_triples_random(g_no_noise, uri, noise_percentage)\n",
    "    noisy_g_random.serialize(destination=f\"datasets/noise/{dataset_name}_random_{noise_percentage}.owl\", format='xml')\n",
    "    \n",
    "    noisy_g_disjoint, new_g_disjoint = add_disjoint_axioms(g, g_no_noise, all_disjoint_classes, all_disjoint_properties, uri, noise_percentage)\n",
    "    noisy_g_disjoint.serialize(destination=f\"datasets/noise/{dataset_name}_disjoint_{noise_percentage}.owl\", format='xml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
